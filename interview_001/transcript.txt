Speaker 1: Is there cool small projects like Archive Sanity and so on that you're thinking about that the world, the ML world can anticipate?  

Speaker 0: There's always like some fun side projects.  Archive Sanity is one.  Basically like there's way too many archive papers.  How can I organize it and recommend papers and so on?  I transcribed all of your podcasts.  

Speaker 1: What did you learn from that experience?  from transcribing the process of, like, you like consuming audio books and podcasts and so on, and here's a process that achieves closer to human level performance on annotation.  

Speaker 0: Yeah, well, I definitely was surprised that transcription with OpenAI's Whisper was working so well, compared to what I'm familiar with from Siri and a few other systems, I guess.  It works so well, and that's what gave me some energy to try it out, and I thought it could be fun to run on podcasts.  It's kind of not obvious to me why Whisper is so much better compared to anything else, because I feel like there should be a lot of incentive for a lot of companies to produce transcription systems, and that they've done so over a long time.  Whisper is not a super exotic model.  It's a transformer.  It takes MEL spectrograms and just outputs tokens of text.  It's not crazy.  The model and everything has been around for a long time.  I'm not actually 100% sure why this came up.  

Speaker 1: Yeah, it's not obvious to me either.  It makes me feel like I'm missing something.  I'm missing something.  Yeah, because there is a huge, even at Google and so on, YouTube transcription.  Yeah.  Yeah, it's unclear.  But some of it is also integrating into a bigger system.  

Speaker 0: Yeah.  

Speaker 1: So the user interface, how it's deployed and all that kind of stuff, maybe running it as an independent thing is much easier, like an order of magnitude easier than deploying to a large integrated system like YouTube transcription or anything like meetings, like Zoom has transcription, that's kind of crappy.  But creating an interface where it detects the different individual speakers, it's able to display it in different ways.  compelling ways, run it in real time, all that kind of stuff.  Maybe that's difficult.  But that's the only explanation I have because I'm currently paying quite a bit for human transcription, human captions, annotation.  And it seems like there's a huge incentive to automate that.  It's very confusing.  

Speaker 0: And I think, I mean, I don't know if you looked at some of the Whisper transcripts, but they're quite good.  

Speaker 1: They're good.  And especially in tricky cases.  I've seen whispers performance on like super tricky cases and it does incredibly well.  So I don't know, a podcast is pretty simple.  It's like high quality audio and you're speaking usually pretty clearly.  And so I don't know, I don't know what OpenAI's plans are either.  

Speaker 0: But yeah, there's always like fun.  Fun projects, basically.  And stable diffusion also is opening up a huge amount of experimentation, I would say, in the visual realm and generating images and videos and movies, ultimately.  

Speaker 1: Yeah, videos now.  

Speaker 0: And so that's going to be pretty crazy.  That's going to almost certainly work, and it's going to be really interesting when the cost of content creation is going to fall to zero.  You used to need a painter for a few months to paint a thing, and now it's going to be speak to your phone to get your video.  

Speaker 1: So if Hollywood will start using that to generate scenes, which completely opens up, yeah, so you can make a movie like Avatar eventually for under a million dollars.  

Speaker 0: Much less, maybe just by talking to your phone.  I mean, I know it sounds kind of crazy.